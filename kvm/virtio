virtio: Towards a De-Facto Standard For Virtual I/O Devices

abstract

Linux内核目前支持至少8种虚拟化系统：Xen, KVM, VMware’s VMI, IBM’s System p, IBM’s System z, User Mode Linux, lguest and IBM’s legacy iSeries.看上去很可能会有更多的这种系统出现，直到最近以上各种系统都有了自己的分段，网络，控制器和其他驱动并有各自不同的特性与优化。

本文意在介绍virtio，一个系列高效，良好维护的linux驱动，可以通过一个中间层被改造成各种不同的管理程序实现。这包括为不同驱动提供一个简单的可扩展的机制。我们也提供一个明显的环缓冲传输实现叫做vring，它被用于KVM和lguest。这给所有新的hypervisor提供了一个阻力最小的路径：支持高效的传输机制会立即减少需要被完成的工作。最后我们会给出一个呈现vring 传输和把设备设置成PCI设备的的实践，这意味着客操作系统仅需要一个新的PCI驱动，同时hypervisor只需要加入vring支持到实现的虚拟的设备（目前只有KVM这样做）。

这篇文章将会描述集成在linux的virtio API 层，vring实现，在PCI驱动种的实施方案。我们将会以一些初期集成这项I/O机制到linux内核的工作为结尾。

1 Introduction

linux内核被用于大量的平台上，正式的内核包含24分离的结构目录，八百四十万行中二百万行是关于结构的代码。这些结构支持各种不同平台变种。不幸的是我们发现只有一种平台删除了树结构，而新的硬件像杂草一样生长。当每天有10000行代码修改，内核有至少一件事你可以想到。

当我们把linux看作一个虚拟化的客机，我们应该感到庆幸IBM’s System p, System z and legacy iSeries都支持。用户模式linux很早就被加入，可以把linux作为一个用户空间进程运行在Power, IA64 and 32 and 64 bit x86 机器上。在过去几年，X86构架被证明收获颇丰，有Xen， VMI， KVM。最后，还有我们的lguest，一个悄然加入的适用于开发教学玩具hyperviosr。

这八种平台都有自己模块，网络，控制台的驱动，甚至帧缓冲，USB控制器，主机文件系统和虚拟厨房沉控制器。其中一些也从某些显著的方面进行了优化，互相重叠但又有一些不同的特质。重要的是，没有一个没有一个特别完满他们的驱动，或者维护他们。

这个问题于KVM息息相关，并从2006年还没有准虚拟化设备模式时，就起引起了广泛关注。 模拟设备性能的局限已经很清楚，
very-Xen-centric模型跟其他正在开发的模型也是一样。我们相信是可以开发一个一般性的高效的能在多个hypyervisor和平台虚拟I/O机制，同时避免Xen设备设置的系统。

2. Virtio： The Three Goals
我们对驱动统一最开始的目标是：所有的工作都在linux内核内，所以不需要借助其他方面的帮助。如果boutique virtual I/O mechanisms开发者熟悉linux，这就能帮助他好的对应linux API与他们的ABI。但是如果这样是低效的，我们有更大的野心。

经验证明，精致的传输机制不仅应对一个hypervisor，一种构架更是一对一个具体类型的设备。所以下一步是企图为一般发布和应用缓冲提供一个统一的ABI。我们的virtio-ang实现有意的不去重新变革。开发者将会喜欢这些代码。

最后，我们提供两套ABI实现：用virtio—ring结构和虚拟I/O设备的linux api。这些是了虚拟I/O的最终部分：设备检测和配置。重要的是，他们展示了用linux虚拟API来提供其后兼容的功能交流是多么简单，所以今后linux驱动能被任何主机实现检测和使用。

明确的区分驱动，传输和配置另一种对实现的想法。例如，你不能在一个新的hypervisor上用Xen‘s linux网络驱动，除非你支持Xen-Bus探测和配置系统。

3. virtio： A LINUX-INTERNAL ABSTRACION API

如果我们向减少重复的虚拟设备驱动，我们一个好的抽象方式使驱动可以共享代码。一个方法是提供一些列虚拟驱动可用的功能助手。更具野心的方案是用通用驱动，和一个执行结构：一系列操作函数指针来处理通用驱动与其他传输实现对接。它的任务是创建一个简单的，接近最优高效的传输，并且能与现有传输很好整合的传输抽象给所有虚拟设备。

目前的结果（结成进2.6.24）是virtio驱动登记自己去处理一个32bit设备类型，限制于一个特定的32bit供应商。这个驱动的探测在一个合适的virtio设备被发现使被调用： struct virtio_device伴随一个virtio_config-ops指针被传进来。驱动于用这个指针来解开设备设置。

配置操作可会议分为四部分，读写特性信息，读西配置空间，读取状态信息和设备重设置。设备寻找于想要使用的特征相对应的指定设备类型特性信息。例如VIRTIO_NET_F_CSUM 特征信息表明一个网络设备是否支持校验和卸载。特征信息明确的被确认：主机直到什么特征信息被客机确认，和那些特性驱动知悉。第二部分是配置空间，这是一个有关虚拟设备的设备星系的结构。它可以被客机读写。例如网络设备VIRTIO_NET_F_MAC特性信息，表明这个主机想要设备有个特定MAC地址，并其配置空间有这些信息。

这个机制使我们有空间增加新特性，对主机来说增加新特性到设备只需要一个特性信息码和确认的配置空间，

有很多层哦呵做来设置读取一个客机用来表明设备探测状态的8bit设备状态词。当VIRTIO_CONFIG_S_DRIVER_OK被设置，它允许客机驱动有有完全的特性探测。这时，主机直到什么特性客机知道并想使用。

最后，重置操作来重置设备，和他的配置与状态信息。这对遇到已经初始化过的，要被再次删除添加的模块化的驱动是必须的。这样也能避免当一个驱动关闭时，从一个设备上除去缓存的问题：当重置缓存后可以确保设备不会改写他们。也可以被用来尝试恢复客机的驱动。

3.1Virtqueues: A Transport Abstraction

我们的API配置很很重要，但是api关键性能部分是I/O机制。我们的抽象化方式是virtqueue，配置操作有一个find_vq，它会根据virtio设备和索引返回提前充满数据的结构队列。有些设备比如virtio 模块设备只有一个队列，但是想像网络，控制台设备有一个输入队列，一个输出队列。

一个virtqueue是一个简单对客机提出对主机需求的缓冲队列。每个缓存是一个包括可读可写的分散聚合数组：数据的结构跟设备类型有关。virtioqueue操作结构如下：
struct virtqueue_ops {
int (*add_buf)(struct virtqueue *vq,
struct scatterlist sg[],
unsigned int out_num,
unsigned int in_num,
void *data);
void (*kick)(struct virtqueue *vq);
void *(*get_buf)(struct virtqueue *vq,
unsigned int *len);
void (*disable_cb)(struct virtqueue *vq);
bool (*enable_cb)(struct virtqueue *vq);
};
对add_buff的调用被用来添加新的缓存队列，数据变量是由驱动在缓存被利用后提供的非空序列。调用kick，可以在缓存被添加时通知其他部分，多个缓存可以在一个kick被配量添加。这很重要，因为同志通常导致大量客机退出状态。

get——buff获取一个用过的缓存：被其他部分返回的长度已经被写入的缓存。它返回一个cookie来处理add——buff或者null：不需要的缓存。

disable——cb是一个当缓存被使用是客机不需要提示：这等于禁止设备的打断。设备记录一个回调函数给virtioqueue当它被初始化。virtioqueue回调也许在启动一个服务线程前禁止特性回调。在这之后并不确定一个回调函数回被调用，但是这会需要大量同步尤其在SMP系统中。实际上，这是一个为了减少不需要的于主机和VM的交互优化。

enable——cb于disable——cb相反，通常一个驱动会在处理完所有队列里的代处理缓存后重新启动回调。在一些virtio传输中会有资源竞争：缓存会被get——bull 和enable——cb利用，或者没有回调函数被调用的时候。电平触发中断实现没有这个问题，但是对那些有这些问题的，当回调被禁止时，enable——cb回返回错误来说明更多工作出现了。

以上这些调用对很多linux情景都是可用的，并且由使用这来确定他们没有被同时调用。只有一个例外是disable——cb，它通常被回调调用，并且用来禁止回调，但是它不可靠，所以可以在任何时间发生。


4 VIRTIO_RING: A TRANSPORT IMPLEMENTATION FOR VIRTIO
机关我们认为任何优化过的传输都有相似的特性，linux virtio/ virtioqueue API是基于我们特有的传输实现，叫做viurtio——ring。

在开发virtio前，lguest已经有了一个虚拟I/O系统：一个通用，N-way I/O 机制，被使用在基于Xen的客机内联网。但是系统的复杂性来自它的广播N-way属性，它看起来只在一些客机LAN被需要。一个有趣的特性是，如果舍弃它能使我们有个更简单的涉及ringbuffers的方案，这正是告诉I/O的标准方案。经过一些开发周期，virtio——ring方案已经使用在lguest和KVM。

virtio——ring有三个组成部分：一个客机将长度和地址联系起来的描述符数组，一个可用的可以表明哪些描述符可用的ring，一个已经使用的ring，主机表明哪些描述符链已经被使用。ring的大小是可变的，但是必须是2次幂。

struct vring_desc
{
__u64 addr;
__u32 len;
__u16 flags;
__u16 next;
};
每个描述符包括缓存的客机物理地址，及其长度，一个可选的作为链接的 ‘next’缓存，和两个flag：一个用来表明下一个field是否有效，一个控制缓存是否只读或者只写。这允许一个串联的缓存包含可读和可写的项目，这对实现于一个模块设备很有用。一般，可读的缓存，优先与可写的缓存。

使用在32bit系统中使用64bit地址是一个妥协：它在旧的平台上实现了一个统一的32bit的格式。所有结构都选择避免避免填充除了一些最保守的机构。但是我们停止了段定义一个特定的endian格式L客机假定他的自然字节顺序。
struct vring_avail
{
__u16 flags;
__u16 idx;
__u16 ring[NUM];
};
一个可用的ring包括一个自由运行的索引，一个打断抑制的flag，一个包含描述符列表索引的数组。把描述符从可有的ring中分离，是因为virtioqueque的异步特性：一个可用的ring可能在快速服务的描述符轮询很多此，同时一直等待慢描述符的结束。这对实现模块设备十分有用，同样对零拷贝网络很有用。

struct vring_used_elem
{
__u32 id;
__u32 len;
};
struct vring_used
{
__u16 flags;
__u16 idx;
struct vring_used_elem ring[];
};

已经使用的ring与可用的rong相似。但是当描述符链被使用时才被主机写入。注意这里有填充，例如放置这个结构在一个与可用的ring与描述符数组分离的page。这很好的提供缓存行为并确保各方只需要一个写入到virtioqueue结构的一部分。

vring——used 和vring——avail flag，他们现在被用来抑制推送消息。例如一个正在被使用的flag，被主机告诉客机没有kick时没有必要添加缓冲，因为kcik需要 vmexit，这是一个重要的优化，kvm实现使用可这种方式，用一个计时器来控制网络传输退出迁移。相同的 avail flag被客机网络驱动使用用来直呼进一步的打断是不需要的。

最后，我们没有给客机感知暂停或恢复的基础设施，我们也不需要这些，因为我们和少发布自己的缓冲。挂起和恢复对kvm的主机的实现只有微不足道的好处。

4.1 A Note on Zero-Copy And Religion of Page Flipping

当时设计高效的I/O是，我们必须注意两件事：每个操作需要的推送消息数，被访问是cache-cold的数据量。前者被virtio——ring打断抑制flga很好的处理着。对cache-cold数据的处理需要进一步讨论。

在kvm和lguest的模式中，客机内存作为主机的进程虚拟内存空间的一部分，对主机os，那个进程是一个客机。因此从客机到主机I/O应该跟其他主机的I/O一样快，只有一些在主客机之间额外的字符转换的消耗。这就是为什么virtio在目标的I/O可以被可以访问内存前提下发布缓冲。

Xen没有这样自然的访问模式：没有这样一个主机需要访问客机的内存，所有的个体都是对等的。这等于kvm和lguest下的内客机交流，为了实现客机间的 zreo-copy，客机间对应缓冲时必要的。

需要拷贝的cache-cold数据量主导着拷贝时间。如果一个客机或主机遇到大量的数据，拷贝时间会被高度分摊。page mapping的消耗于数据量是相互独立的。但是这只针对page-aligned page-sized的数据，基于此，这只有面对大量数据时才有意义。

通常‘page-flipping’方案在各个内客机I/O涉及两个分离的page table改变：一个是对应，另一个取消对应。我们必须确定确保缓冲在释放前已经被取消对应，否则当一个page还在被其他客机使用时，就可能被回收作其他用处。应为完成的通知可以批量处理或者推迟，因此消耗可以被分摊。但是在SMP系统上，这种操作依然消耗很大。

永久的共享一个段固定的内存能避免 page flip，但是并不适合客机系统例如glinux的目标：因为如果我们可以从一个短固定的内存读取数据，那么其他方面也可以，因此我可以只是从其他的客机来读取。

现在很少有大量的客机间的复制， 因为TSO的实现，virtio——net被限制成64k的数据包，客机件的模块设备就是这样一个不起眼的用例。尽管如此，证明page－flipping的价值是一个简单的代码问题，而我们怀疑的结果是没有意义的的，我们希望一些发烧友能够挑战我们并以证明我们的错误。

这项工作可能永远不能完成的愿意是，即将使用的用以复制大量数据的DMA引擎。他们与page－flipping有相似的好处，大量cache－cold传输。

5. CURRENT VIRTIO DRIVERS

现在我们熟悉了linux virtio 和virqueque的概念，以及其api和一种传输的实现。参考一些现有的virtio驱动是很有价值的。因为kvm模拟出控制台，我们有一个简单笨拙的lguest控制台驱动，直到有人发布一些类似 virtcon benchmark的工具，控制台的性能才引起人们的注意。

我们还有一个ballon驱动能使主机规定他想从客机获取的page的数量。客机传入page 数的数组给主机，主机可以取消这些page的对应，并在它们被存取时把它们替换成0page。

我们将会深入的讨论两个普遍而重要的驱动，模块和网络驱动。

5.1 Virtio Block Driver

对于模块驱动，我们有一个简单的请求队列。在队列中的每个缓冲的前16字节都是只读的描述符。
struct virtio_blk_outhdr{    __u32 type;    __u32 ioprio;    __u64 sector;};
类型表示他是否是读取，写入或者通用SCSI命令，并且是否有一个写入屏障优先于这个命令。IO优先级允许客机提示请求的相对优先级。这一项被所有现有实现适当的忽略了。扇区是512字节的读取或写入的偏移量。

描述符除了一个字节外的其它字节的不是只读就是只写，这取决于请求的类型，其长度决定了请求的大小。最后一个字节是只写，表明该请求成功（0），失败（1）不支持（2）。

模块驱动支持屏障和简单的scsi命令（主要用于拔除虚拟CDROMs）对于更复杂的应用，一个在virtio上的SCSI HBA应该被安装。

5.1.1 Virtio Block Mechanics

为了更好的聚焦这个机制，我们将会用virtio——ring作为传输的例子来详述virtio模块驱动遍历来执行单一模块读取的过程。最开始，客机有一个将被读入数据的空缓冲。我们通过请求元数据来定位 struct virtio_blk_outhdr，一个字节来读取状态（成功或失败），如图2所示。

我们把请求的3部分转化成写入描述符表3步，并把它们串联起来。在这个例子中，我们读取的缓冲是物理连续的：如果不是，我们将使用多步读取描述符表。头部是只读的，空缓冲和状态字节时只写的，如图3所示。

当这一步完成，描述符	就会被标记为可用，如图4所示。这是因为把描述符头的索引放入了可用的ring，并设置一个内存障碍，然后将可用索引加一。一个kick被给出来通知主机有一个待处理请求(实际上我们的驱动江所有待处理请求放置在ring中，然后给出一个kick)。

在接下来某一时刻，请求会如图5所示完成：缓冲被填满，，状态字节更新为成功。到这时，用过的ring返回描述符头，并通知客机。模块驱动回调函数get－buf不停的查看哪些请求结束了，直到返回null。



5.2 Virtio Network Driver

网络设备使用2个队列：一个用来发送，一个用来接收。与模块驱动一样，每个网络缓冲都被先处理头部，然后卸载校验和TCP/UDP分段。分段卸载被开发给网络硬件不大与1500字节的传输包来实现大量MTU。越少的数据包，表示越少的PCI传输。在虚拟环境中，这意味这虚拟机更少的调用，性能也随之提高。
struct virtio_net_hdr
{
// Use csum_start, csum_offset
#define VIRTIO_NET_HDR_F_NEEDS_CSUM
__u8 flags;
#define VIRTIO_NET_HDR_GSO_NONE
#define VIRTIO_NET_HDR_GSO_TCPV4
#define VIRTIO_NET_HDR_GSO_UDP
#define VIRTIO_NET_HDR_GSO_TCPV6
#define VIRTIO_NET_HDR_GSO_ECN
__u8 gso_type;
__u16 hdr_len;
__u16 gso_size;
__u16 csum_start;
__u16 csum_offset;
};

在2.6.24内核中的Virtio网络驱动有一些可以TSO传入数据的结构，但是它不能定位大量输入的缓冲，所以不能被使用。当我们强调向前的兼容性时，我们将会讲解如何作出这个变化。

值得注意的时，网络驱动压制在传输virtqueque的回调函数，与模块驱动不同的时，它不ing不在乎数据包是否完成。除了队列慢的时候：驱动重启回调函数，来保证当缓冲被使用后，它可以继续传输。

6. VIRTIO_PCI: A PCI IMPLEMENTATION OF VRING AND VIRTIO

目前，我们已经介绍了2种统一虚拟I/O的方法。第一，通过在linux内核种使用virtio驱动，并提供合适ops结构来驱动特定传输。第二，通过使用virtio——ring模式作为它们的传输。我们现在将讨论为了实现完全虚拟I/O ABI的设备的侦测与配置。

大多数全虚拟化主机已经有一些形式的PCI仿真并且大多数客机都有一些方法来添加新的第三方PCI驱动。所以我们需要提供一个能够给主机和客机最大化兼容标准的 virtio-over-PCI的定义。这是一个相当直接的vring实现和使用I/O的配置。例如，virtio——pci 网络驱动使用Linux的 virtio net API 中的struct virtio_net_hdr作为ABI，直接传入头部给主机。这样的结构是有意被设计出来的，这样使得传输变得更加简单。

启动KVM的Qumranet捐赠了他们的从0x1000到0x10FF的设备ID。PCI设备的子系统供应商和设备id变成virto类型和供应商字段，因此PCI驱动不需要知道virtio类型的意义；在linux中这表示创建一个struct virtio_device并在virtio bus登记它，使得virtio驱动可以识别。

在有些平台上，I/O空间可能需要特殊的存储器。但是理论上，它有以下结构：
struct virtio_pci_io
{
__u32 host_features;
__u32 guest_features;
__u32 vring_page_num;
__u16 vring_ring_size;
__u16 vring_queue_selector;
__u16 vring_queue_notifier;
__u8 status;
__u8 pci_isr;
__u8 config[];
}
发送接收数据的特性是I/O空间中两个32字节的字段，当需要时，最后一位可以被扩充。vring_queue_selector 被用来访问设备的virtqueque：如果vring_ring_size为0，这个对列就不存在。否则客机将会写入队列指定的地址的vring_page_size：这与主机为ring定位空间的lguest不同。

当一个队列中有新的缓冲时，vring_queue_notifier被用来通知主机， status自己被用来写入标准的virtio状态码，0用来重置设备。pci_isr字段有个清除打断读取的副作用，非零表示其中一个virtqueuque正在等待，第二位表示设备的配置被改变了。

ring有客机物理的地址：对kvm和lguest来说，这些地址与主机进程的虚拟内存有一个简单差别。这样主机只需要检查地址没有超出客机的内存，然后直接应用这个差别来处理readv和writev:如果他们遇到一个内存洞，它将回给出一个-EFAULT，并且主机价格返回一个错误给客机。

最后，实现PCI virtio驱动时一个比较简单的实践，它在2.6.25内核中大概有450行，或者170句。

7. PERFORMANCE

遗憾的是，除了一些测试来保证我们的性能并不糟糕，在现阶段几乎没有可用的性能数据。没有明显障碍阻止在现代linux客机运行在现代linux主机的情况下，我们创造一个虚拟I/O的记录目标。像其他人一样，随着硬件支持的提高，我们期望实现一个裸机速度。

网络性能现在备受关注：实现多种TSO选项是现在的优先项。因为它除去在QEMU框架下KVM中出现的复制。一旦这个完成，我们期待启发式压制通知能够得到更多关注：这入标准的高性能NIC一样，我们的linux驱动将会在接收信息时进入轮询模式，但是在传输包时减少通知的方法仍然原始。

8. ADOPTION

目前KVM和lguest都使用virtio作为它们内置传输；对KVM意味着在32和64位x86机器，System z (ie. S/390), IA64 和实验性的 PowerPC支持linux virtio驱动。lguest只时32为X86，但是接下来的几年的布丁也许会扩展至64位。

Qumarnet已经为Windows客机发布virtio pci Windows 驱动。KV名使用QEMU模拟器来来支持设备模拟，并且KVM版的virtio是主机支持的，但是并未优化。这里还有很多工作需要继续。lguets的启动程序也只支持很少的virtio实现。我们没有注意到其他能后用来增加最后一些性能百分比的实现。

因为virtio非常容易开发，我们希望看到更多在其他平台上的virtio客机驱动。

8.1 Adapting Existing Transports to Use Virtio Drivers

你将会注意到kvm和lguest驱动都在缓冲前设置了一个定义好的头部:这看起来很想ABI，因为直接传给主机，确实对他们来说是这样的。

但是，virtio驱动一个关键目标是允许他们能在不同的传输上工作。这需要修改配置很设备可见的属性字段。例如，如果一个网络驱动被告知主机不能支持TSO或者校对卸载，整个网络头部会在传输时被忽略，并不接收数据。这会发生在add_buf回调中。如果头部的格式不同，或者相似的信息被以其他形式发送，它就会被打断。

一个摆在我们面前的任务是为hypervisor创建一个能够进行测试和提供基准的中间层，并最终希望其他维护这使用virto驱动。Xen驱动是最具挑战的：它们不仅已经被优化过了，而且它是一个相当完善的支持virto驱动不支持的可断连重连的特性。

替代现有驱动只有很少的好处，但是我们相信在两种情况下，使用virtio架构是有说服力的，第一个是当虚拟化技术加入一个已经被virtio支持的新的虚拟设备，已经支持其实重新写一个新的是更小的负担。例如，已经有能够提高客机随机属性的virtio熵驱动，将会被加入到linux 2.6.27.

第二种情况是，当新的虚拟化传输想支持linux，我们希望它能够直接使用vring，如果不行，我们至少希望能够得到现有的驱动，而不是实现和支持它们并不在行的linux驱动。

9.FUTURE WORK

尽管ABI已经是稳定版本，virtio和其驱动还在持续开发中，新特性还会被持续添加和优化。我们将会详细描述一些可兼容的新特性，一些实验现在正在进行来提供将要添加新特性的信息。

9.1 Feature Bits and Forward Compatibility

当然不是所有的主机都将支持所有的功能，因为与它们不是太旧了就是它们不支持校对卸载和TSO。

我们知道特性位机制，但是它的实现也值得一提，lguest和virtio_pci使用来两个bitmap，一个是主机提供的特性，另一个是驱动接收的特性。当被设置位VIRTIO_CONFIG_S_DRIVER_OK状态，主机可以检查接收的特性，来判断客机驱动是否能够支持。

目前所有的定义好的特性与特定的设备类型相关，例如表明主机支持模块驱动障碍。我们给一些与设备无关的特性，预留了一些字段。尽管我们不想随机的添加特性字段，因为这样使得主客机交流变得更加复杂，我们仍然允许实验。

9.2 Inter-guest Communication

使主机支持客机内交流还是很容易的，也确实有一个实验性质的lguest补丁就是实现这一功能。这个客机启动进程把其他客机和自己的内存对应起来，并通过一个管道来通知其他启动程序的客机内I/O。客机决定加入哪个virtioqueque，然后的步骤很简单：从这个客机的virtioqueque中获取一个缓冲，其他的客机的virtioqueque和基于它们之间的读/写flag的memcpy 数据。因为virtio-net协议是对称，它不需修改就可以支持点对点客机内网络链接。如果提供服务的客机可能够支持，相同的代码能够允许一个客机为另一个客机提供一个模块或者控制台驱动。

这对互相不信任的用户之间的交流是一个微妙又重要的协议。 设想这一个客机内网络协议，一个客机接收到了一个宣称是1514字节的数据包，如果这1515自己没有完全复制下来，接收的客机会把缓冲中的旧数据当作余下的部分接收。这个数据会泄漏用户空间，或者从该客机转发。为了防止这样的情况，一个客机将会消杀所有它接收的缓冲，这会是很复杂或者低效的。

这就是为什么在vring中使用的ring有长度字段。只要复制的数量是被可信的源写入，我们就可以避免。在lguets的原型中，启动程序来做这个复制，所以是可信的。一个只链接到主机或者其他可信的源virtio传输实现也能提供这个长度字段。如果一个传输链接到了一个不可信的源，并没有办法知道复制进来的长度，它不许在暴露它们给其他端之前清零缓冲。这会比从驱动中获取更安全，尤其是经过考虑的传输也不存在这个问题。

当一个主机可以在客机间i/O时，把两个vring合并，谈判特性更加困难。我们想提供给每个客机所有的特性，这样它们能更好的利用例如校对卸载和TSO这些特性。但是如果一个客机拒绝了一个我们已经提供的特性，我们或者需要对每个I/O做一些转化，或者热插把设备，并提供更少的特性。

我们考虑的一个解决方案是加入一个‘多轮’谈判特性：如果客机确认了这个特性，在驱动通过设置状态字段来完成特性确认后，它期待特性被重新呈现，直到‘多轮’特性缺失。我们通过在开始呈现一个最小化的特性组来使用这一个特性。通过‘多轮’位，客机双方都确认它，我们会呈现所有可能的特性，并持续除去不被一方接受的特性，直到双方都满意为止。这将是我们第一个非特定设备的特性位。

9.3 Tun Device Vring Support

在qemu/kvm和lguets中的用户空间virtio主机使用linux tun设备；这是一个用户空间终结的网络接口，在其中驱动read（）的传入以太网数据包，write（）发出的数据包。我们发布了一个简单的补丁到这一设备用以支持virtio头部，来使我们测试virtio网络驱动的GSO支持，Anthony Liguori 和 Herbert Xu 修改了它以支持KVM。根据Herbert所说，其性能从一个客机传输TCP到主机与xen下的全虚拟化相近，速度只有客机回溯设备的一半。

这一版会在传输过程中传输数据包两次，一次在QEMU中现性化缓冲，一次在内核中。前者是qemu你在结果限制，后者则更加难以解决。如果我们想避免复制数据缓冲，我们必须访问用户空间pages并只完成写入操作当他们不在被数据包所引用。鉴于在本地接口接收的数据包或者用于non-GSO-capable设备的大量被分割的数据包被修改，创建这样一个析构回调函数并不是完全不重要的。此外，他可能需要任意长的所见来完成，一个数据包可能一直在接口中，如果进程没有读取它。

幸运的是，我们相信我们已经找到了一个解决方案：vring，它用来处理乱序缓冲完成，并且是一个高效的，很好定义过的ABI，因此我们有一个‘dev/vring’补丁来创建一个在用户内存中与vring的ringbuffer相关联的文件描述符。这个文件可以被轮询，读取，写入。最后一个小补丁加入了一些方法来添加这些vringfds来接收和传输tun设备。

我们也实现了一个ioctl来设置vring可访问的offset和关联，这样客机的网络vring可以直接被暴露在主机的内核的tap设备中。最终对效率的实验是用来避免用户空间，并且当其他部分已经就位时，这非常容易实现。

10.CONCLUSIONS

当我们了解更多虚拟I/O解决方案，他们的共性很容易被辨别：一个ring缓冲，一些通知，一些特性位。实际上，他们看起来很像高速物理设备：你设想他们可以向的你内存DMA，并且你与他们尽可能少的交流。这一实现的质量变得更加有趣，表明是时候来创建一个质量实现来确认这是符会成为一个标准。

你也注意到virtio的平凡：它具有打断，设备特性，配置空间，和DMA ring。对驱动开发者来说这些是很熟的概念。操作系统的设备结构也被设计成支持他们。如果virtio驱动成为linux虚拟环境的常态，它将连接虚拟I/O设计进程的一端。没有这一指导，任意的不同随意的出现，集成进客机os成为后话，并使驱动看起来好像来自另外一个宇宙，使得其变得不可读。

虚拟I/O仍有许多需要方面需要完成，但是我们建立起一个坚实的基础，这一进程将会被加速，而不是在没有意义的配置和bit-publishing中越陷越深。我们希望virtio位相信自己可以完成一个虚拟I/O系统的人可以建立一个标准，并在未来启发各种各样的虚拟设备。
